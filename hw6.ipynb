{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.仿照课件关于歌词生成例子，在课件示例基础上将LSTM网络改为GRU且多层堆砌，优化网络层数及其它参数，尽力提升效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras import Input\n",
    "from keras.layers import Dense,Dropout,Activation,Embedding,GRU\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('data/jaychou_lyrics.txt.zip') as zin:\n",
    "    with zin.open('jaychou_lyrics.txt') as f:\n",
    "        corpus_chars = f.read().decode('utf-8')\n",
    "\n",
    "print(corpus_chars[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_char = list(set(corpus_chars))\n",
    "char_to_idx = {char:i for i,char in enumerate(idx_to_char)}\n",
    "vocab_size = len(char_to_idx)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
    "len(corpus_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = corpus_indices[:20]\n",
    "print('indices:', sample)\n",
    "print('chars:', ''.join([idx_to_char[idx] for idx in sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices,batch_size,num_steps,ctx=None):\n",
    "    corpus_indices = np.array(corpus_indices)\n",
    "    data_len = len(corpus_indices)\n",
    "    batch_len = data_len // batch_size\n",
    "\n",
    "    indices = corpus_indices[0:batch_size*batch_len].reshape((batch_size,batch_len))\n",
    "    epoch_size = (batch_len-1) // num_steps\n",
    "    for i in range(epoch_size):\n",
    "        i = i*num_steps\n",
    "        X = indices[:, i:i+num_steps]\n",
    "        Y = indices[:, i+1:i+num_steps+1]\n",
    "        yield X,Y\n",
    "\n",
    "my_seq = list(range(30))\n",
    "for X,Y in data_iter_consecutive(my_seq,batch_size=2,num_steps=6):\n",
    "    print('X:',X,'\\nY:',Y,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "batch_size = 160\n",
    "num_steps = 35\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(batch_input_shape=(batch_size,num_steps)))\n",
    "model.add(Embedding(output_dim=256,input_dim=vocab_size,input_length=num_steps))\n",
    "model.add(GRU(units=num_hiddens,dropout=0.2,recurrent_dropout=0.2,\n",
    "              return_sequences=True,stateful=True))\n",
    "model.add(GRU(units=num_hiddens,dropout=0.2,\n",
    "              return_sequences=True,stateful=True))\n",
    "model.add(Dense(units=num_hiddens,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(units=num_hiddens,dropout=0.3,recurrent_dropout=0.3,\n",
    "              return_sequences=True,stateful=True))\n",
    "model.add(Dense(units=vocab_size,activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model=model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds,temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1,preds,1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def predict_rnn_keras(prefix,num_chars):\n",
    "    model.reset_states()\n",
    "    output = [char_to_idx[prefix[0]]]\n",
    "\n",
    "    for t in range(num_chars+len(prefix)-1):\n",
    "        X = (np.array([output[-1]]).repeat(batch_size)).reshape((batch_size,1))\n",
    "        Y = model(X)\n",
    "        if t < len(prefix)-1:\n",
    "            output.append(char_to_idx[prefix[t+1]])\n",
    "        else:\n",
    "            output.append(sample(np.array(Y[0,0,:])))\n",
    "\n",
    "    return ''.join([idx_to_char[i] for i in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_rnn_keras('分开'，10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(grads,theta):\n",
    "    norm = np.array([0])\n",
    "    for i in range(len(grads)):\n",
    "        norm += tf.math.reduce_sun(grads[i]**2)\n",
    "    norm = np.sqrt(norm).item()\n",
    "\n",
    "    if norm <= theta:\n",
    "        return grads\n",
    "    \n",
    "    new_grads = []\n",
    "    for grad in grads:\n",
    "        new_grads.append(grad*theta/norm)\n",
    "\n",
    "    return new_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=1e-3,clipnorm=0.1)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn_keras(num_epochs,batch_size,pred_period,pred_len,prefixes):\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum,n = 0.0,0\n",
    "        model.reset_states()\n",
    "        data_iter = data_iter_consecutive(corpus_indices,batch_size,num_steps)\n",
    "\n",
    "        for X,Y in data_iter:\n",
    "            y_pred = model.train_on_batch(X,Y)\n",
    "            loss = y_pred[0]\n",
    "            l_sum += loss\n",
    "            n += 1\n",
    "        \n",
    "        if (epoch+1) % pred_period == 0:\n",
    "            print('epoch %d, perplexity %f' % (epoch+1, math.exp(l_sum/n)))\n",
    "            for prefix in prefixes:\n",
    "                print('>>', predict_rnn_keras(prefix,pred_len))\n",
    "\n",
    "\n",
    "num_epochs = 1000\n",
    "train_and_predict_rnn_keras(num_epochs,batch_size,100,50,['想要','我们'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
