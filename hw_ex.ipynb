{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了提升效果，本示例程序将英文原按字符分割改进为按单词分割。\n",
    "请在理解seq2seq代码基础上，完成代码改造工作，提高性能。要求：\n",
    "1）文本分割处理：注意简单空格分割会导致标点符号没有分离，标点符号有意义应作为单独token。非标点符号删除（isn't的引号如何处理可斟酌）\n",
    "2）模型结构：改为GRU单元，多层堆砌，参数优化：神经元数量 等优化  \n",
    "3）训练过程保存最佳模型用于预测  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense,GRU,Bidirectional\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_UNITS = 128\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 50\n",
    "NUM_SAMPLES = 21000\n",
    "embedding_dim = 256"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/cmn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(data_path,header=None).iloc[:NUM_SAMPLES,:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['inputs','targets']\n",
    "df['targets'] = df['targets'].apply(lambda x: '\\t'+x+'\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df.inputs.values.tolist()\n",
    "target_texts = df.targets.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理英文句子\n",
    "def preprocess_sentence(w):\n",
    "    \"\"\"处理英文句子\n",
    "        1,符号左右加空格;\n",
    "        2,将非字母和非标点符号的字符替换为空格;\n",
    "        3,空格去重;\n",
    "    \"\"\"\n",
    "    #在这里写相关处理代码。。。\n",
    "    for p in string.punctuation:\n",
    "        if p in w and p!=\"'\":\n",
    "            w = w.replace(p,' '+p+' ')\n",
    "    for i in w:\n",
    "        if i not in string.punctuation and i not in string.ascii_letters:\n",
    "            w = w.replace(i,' ')\n",
    "    \n",
    "    w = ' '.join(w.split())\n",
    "    return w\n",
    "\n",
    "input_texts=[ preprocess_sentence(t) for t in input_texts]\n",
    "input_texts[-1]\n",
    "preprocess_sentence(\"I'm ok.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4,加载数据;使用tokenizer做词嵌入;分割数据为train,vaild,test数据集\n",
    "def max_length(tensor):\n",
    "    \"\"\"找到数据集中padding的最大值\"\"\"\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang, china = False):\n",
    "    \"\"\"将数据集做padding\"\"\"\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', char_level = china)\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts, china=True)\n",
    "max_length_targ= max_length(target_tensor)\n",
    "max_length_targ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PADDING字符占用一个码，需要+1\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_inp =  max_length(input_tensor)\n",
    "max_length_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6,测试数据转化结果 \n",
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "            \n",
    "print(\"Input Language; index to word mapping\")\n",
    "convert(inp_lang_tokenizer, input_tensor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target Language; index to word mapping\")\n",
    "convert(targ_lang_tokenizer, target_tensor[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 5,拆分训练集和验证集\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=1000)\n",
    "\n",
    "# 打印数据集长度 - Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder的label tensor\n",
    "output_tensor_train=np.zeros_like(target_tensor_train)\n",
    "output_tensor_train[:,:-1]=target_tensor_train[:,1:]\n",
    "\n",
    "output_tensor_val=np.zeros_like(target_tensor_val)\n",
    "output_tensor_val[:,:-1]=target_tensor_val[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor.shape, target_tensor.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型（作业要点）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_input,n_output,n_units):\n",
    "    #训练阶段\n",
    "    #encoder\n",
    "    encoder_input = Input(shape = (None, ))\n",
    "    embeddin = keras.layers.Embedding(n_input, embedding_dim)\n",
    "    #encoder输入维度n_input为每个时间步的输入xt的维度，这里是用来one-hot的英文字符数\n",
    "    # encoder = LSTM(n_units, return_state=True)\n",
    "    encoder = Bidirectional(GRU(n_units,return_state=True,return_sequences=True,dropout=0.2))\n",
    "    #n_units为LSTM单元中每个门的神经元的个数，return_state设为True时才会返回最后时刻的状态h,c\n",
    "    _,encoder_h,encoder_c = encoder(embeddin(encoder_input))\n",
    "    encoder_state = [encoder_h,encoder_c]\n",
    "    #保留下来encoder的末状态作为decoder的初始状态\n",
    "    \n",
    "    #decoder\n",
    "    decoder_input = Input(shape = (None, ))\n",
    "    embeddout = keras.layers.Embedding(n_output, embedding_dim)\n",
    "    #decoder的输入维度为中文字符数\n",
    "    # decoder = LSTM(n_units,return_sequences=True, return_state=True)\n",
    "    decoder = Bidirectional(GRU(n_units,return_state=True,return_sequences=True,dropout=0.2))\n",
    "    #训练模型时需要decoder的输出序列来与结果对比优化，故return_sequences也要设为True\n",
    "    decoder_output, _, _ = decoder(embeddout(decoder_input),initial_state=encoder_state)\n",
    "    #在训练阶段只需要用到decoder的输出序列，不需要用最终状态h.c\n",
    "    decoder_dense = Dense(n_output,activation='softmax')\n",
    "    decoder_output = decoder_dense(decoder_output)\n",
    "    #输出序列经过全连接层得到结果\n",
    "    \n",
    "    #生成的训练模型\n",
    "    model = Model([encoder_input,decoder_input],decoder_output)\n",
    "    #第一个参数为训练模型的输入，包含了encoder和decoder的输入，第二个参数为模型的输出，包含了decoder的输出\n",
    "    \n",
    "    #推理阶段，用于预测过程\n",
    "    #推断模型—encoder\n",
    "    encoder_infer = Model(encoder_input,encoder_state)\n",
    "    \n",
    "    #推断模型-decoder\n",
    "    decoder_state_input_h = Input(shape=(n_units,))\n",
    "    decoder_state_input_c = Input(shape=(n_units,))    \n",
    "    decoder_state_input = [decoder_state_input_h, decoder_state_input_c]#上个时刻的状态h,c   \n",
    "    \n",
    "    decoder_infer_output, decoder_infer_state_h, decoder_infer_state_c = decoder(embeddout(decoder_input),initial_state=decoder_state_input)\n",
    "    decoder_infer_state = [decoder_infer_state_h, decoder_infer_state_c]#当前时刻得到的状态\n",
    "    decoder_infer_output = decoder_dense(decoder_infer_output)#当前时刻的输出\n",
    "    decoder_infer = Model([decoder_input]+decoder_state_input,[decoder_infer_output]+decoder_infer_state)\n",
    "    \n",
    "    return model, encoder_infer, decoder_infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_inp_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_train, encoder_infer, decoder_infer \u001b[39m=\u001b[39m create_model(vocab_inp_size, vocab_tar_size, N_UNITS)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_inp_size' is not defined"
     ]
    }
   ],
   "source": [
    "model_train, encoder_infer, decoder_infer = create_model(vocab_inp_size, vocab_tar_size, N_UNITS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_tensor, target_tensor\n",
    "\n",
    "model_train.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "savepath = 'save/3200102349/ex_train.h5'\n",
    "checkpoint = ModelCheckpoint(filepath=savepath,monitor='val_acc',\n",
    "                             verbose=0,save_best_only=True,save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.fit([input_tensor_train, target_tensor_train],output_tensor_train,batch_size=BATCH_SIZE,epochs=EPOCH,\n",
    "                validation_data=([input_tensor_val, target_tensor_val],output_tensor_val),callbacks=[checkpoint] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_infer.load_weights(savepath,by_name=True)\n",
    "decoder_infer.load_weights(savepath,by_name=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_chinese(source,encoder_inference, decoder_inference, n_steps, features):\n",
    "    #先通过推理encoder获得预测输入序列的隐状态\n",
    "    state = encoder_inference.predict(source)\n",
    "    #第一个字符'\\t',为起始标志\n",
    "    predict_seq = np.zeros((1,1))\n",
    "    predict_seq[0,0]=1   ##target_dict['\\t']\n",
    "\n",
    "    output = ''\n",
    "    #开始对encoder获得的隐状态进行推理\n",
    "    #每次循环用上次预测的字符作为输入来预测下一次的字符，直到预测出了终止符\n",
    "    for i in range(n_steps):#n_steps为句子最大长度\n",
    "        #给decoder输入上一个时刻的h,c隐状态，以及上一次的预测字符predict_seq\n",
    "        yhat,h,c = decoder_inference.predict([predict_seq]+state)\n",
    "        #注意，这里的yhat为Dense之后输出的结果，因此与h不同\n",
    "        char_index = np.argmax(yhat[0,-1,:])\n",
    "        if char_index>0:\n",
    "            char = targ_lang_tokenizer.index_word[char_index]\n",
    "        else:\n",
    "            char = ''\n",
    "        output += char\n",
    "        state = [h,c]#本次状态做为下一次的初始状态继续传递\n",
    "        predict_seq = np.zeros((1,1))\n",
    "        predict_seq[0,0]=char_index\n",
    "        if char == '\\n':#预测到了终止符则停下来\n",
    "            break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertinp(tensor):\n",
    "    s=''\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            s += inp_lang_tokenizer.index_word[t]+' '\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    test = input_tensor_val[i:i+1,:]#i:i+1保持数组是三维\n",
    "    out = predict_chinese(test,encoder_infer,decoder_infer,max_length_targ,vocab_tar_size)\n",
    "\n",
    "    print(convertinp(input_tensor_val[i]))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = input_tensor_val[5:6,:]#i:i+1保持数组是三维\n",
    "if 0 in test:\n",
    "    pos0 = np.where(test[0]==0)\n",
    "    test[0] = np.delete(test[0],pos0)\n",
    "    print(test)\n",
    "out = predict_chinese(test,encoder_infer,decoder_infer,max_length_targ,vocab_tar_size)\n",
    "\n",
    "# print(convertinp(input_tensor_val[i]))\n",
    "# print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
